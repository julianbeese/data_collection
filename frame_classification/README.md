# Frame Classification fÃ¼r Brexit-Debatten

Dieses Projekt ermÃ¶glicht die Klassifikation von politischen Reden in verschiedene Frames mithilfe von LLM-Fine-Tuning.

## Frame-Kategorien

- **Human Impact**: Fokus auf menschliche Auswirkungen, persÃ¶nliche Geschichten, Betroffenheit
- **Powerlessness**: GefÃ¼hl der Ohnmacht, fehlende Kontrolle, Hilflosigkeit  
- **Economic**: Wirtschaftliche Argumente, Kosten, Nutzen, Finanzen
- **Moral Value**: Ethische/moralische Argumente, Werte, Prinzipien
- **Conflict**: Konflikt, Opposition, Widerstand, Kampf
- **Other**: Sonstige Kategorien, die nicht in die obigen passen

## Projektstruktur

```
frame_classification/
â”œâ”€â”€ data/                    # Verarbeitete Daten
â”œâ”€â”€ scripts/                 # Python-Scripts
â”‚   â”œâ”€â”€ load_speeches.py     # LÃ¤dt Reden und erstellt Chunks
â”‚   â”œâ”€â”€ annotation_interface.py  # Interaktive Annotation
â”‚   â”œâ”€â”€ generate_training_data.py  # Generiert Training-Daten
â”‚   â””â”€â”€ fine_tuning_pipeline.py   # Fine-Tuning Pipeline
â”œâ”€â”€ annotations/             # Annotation-Ergebnisse
â”œâ”€â”€ models/                  # Trainierte Modelle
â””â”€â”€ README.md
```

## Workflow

### 1. Daten vorbereiten

```bash
# Lade Reden aus gefilterter Datenbank und erstelle Chunks
python scripts/load_speeches.py \
    --chunks ../../data/processed/debates_brexit_filtered.duckdb \
    --output-dir data \
    --chunk-size 500 \
    --overlap 50
```

### 2. Chunking durchfÃ¼hren

#### Intelligentes Datenbank-Chunking

```bash
# Erstelle intelligente Chunks in der Datenbank
python scripts/simple_database_chunking.py --max-speeches 1000

# FÃ¼r alle Daten (kann lange dauern)
python scripts/simple_database_chunking.py
```

**Chunking-Features:**
- ğŸ§  Semantisches Chunking mit spaCy
- ğŸ“Š Optimale Chunk-GrÃ¶ÃŸe (100-150 WÃ¶rter)
- ğŸ—„ï¸ Direkte Speicherung in DuckDB
- ğŸ‘¤ User-Zuweisung fÃ¼r Annotation
- ğŸ“ˆ VollstÃ¤ndige Metadaten

### 3. Annotation durchfÃ¼hren

#### Datenbank-basiertes Streamlit Interface (Empfohlen)

```bash
# Starte Streamlit Annotation Interface
python start_annotation_db.py
```

**Streamlit Interface Features:**
- ğŸŒ Web-basiertes Interface (http://localhost:8501)
- ğŸ—„ï¸ Direkte Datenbank-Integration
- ğŸ“Š Live-Statistiken und Fortschrittsanzeige
- ğŸ’¾ Automatisches Speichern in DB
- ğŸ”„ Pausieren und spÃ¤ter weitermachen
- ğŸ“ˆ Frame-Verteilung Charts
- ğŸ‘¤ User-Management
- ğŸ“¥ CSV/JSON-Export fÃ¼r Backup
- ğŸ§­ Einfache Navigation zwischen Chunks

#### Option B: Terminal Interface

```bash
# Starte Terminal Annotation
python scripts/annotation_interface.py \
    --chunks data/speech_chunks.json \
    --output annotations/annotations.json
```

**Terminal Interface Features:**
- Zeigt Chunks mit Metadaten an
- Einfache Tastatur-Navigation
- Speichert Fortschritt automatisch
- Statistiken und Verteilung

**Kommandos:**
- `1-6`: Frame-Kategorie auswÃ¤hlen
- `s`: Speichern und beenden
- `q`: Beenden ohne speichern
- `n`: NÃ¤chster Chunk
- `p`: Vorheriger Chunk
- `j <nummer>`: Springe zu Chunk
- `r`: Chunk Ã¼berspringen

### 3. Training-Daten generieren

```bash
# Generiere Training-Daten in verschiedenen Formaten
python scripts/generate_training_data.py \
    --annotations annotations/annotations.json \
    --output-dir data/training_data \
    --test-size 0.2
```

**Generierte Formate:**
- `train.jsonl` / `test.jsonl`: JSONL-Format fÃ¼r Classification
- `train.csv` / `test.csv`: CSV-Format
- `train_alpaca.jsonl` / `test_alpaca.jsonl`: Alpaca-Format fÃ¼r Instruction-Following
- `few_shot_examples.json`: Beste Beispiele pro Kategorie
- `prompt_template.txt`: Prompt-Template fÃ¼r Few-Shot

### 4. Fine-Tuning durchfÃ¼hren

#### Option A: Hugging Face (Empfohlen)

```bash
# Bereite Hugging Face Fine-Tuning vor
python scripts/fine_tuning_pipeline.py \
    --training-data data/training_data \
    --output-dir models \
    --approach huggingface \
    --model-name distilbert-base-uncased

# FÃ¼hre Fine-Tuning aus
python models/hf_fine_tuning.py
```

#### Option B: OpenAI

```bash
# OpenAI Fine-Tuning (erfordert API-Key)
export OPENAI_API_KEY="your-api-key"

python scripts/fine_tuning_pipeline.py \
    --training-data data/training_data \
    --output-dir models \
    --approach openai
```

### 5. Modell evaluieren

```bash
# Evaluierung des trainierten Modells
python models/evaluate_model.py \
    --model-path models/frame_classification_model \
    --test-data data/training_data/test.jsonl \
    --output results/evaluation.json
```

## Verwendung des trainierten Modells

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# Lade Modell
tokenizer = AutoTokenizer.from_pretrained("models/frame_classification_model")
model = AutoModelForSequenceClassification.from_pretrained("models/frame_classification_model")

# Klassifiziere Text
def classify_frame(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        predicted_class_id = predictions.argmax().item()
        confidence = predictions.max().item()
    
    id2label = model.config.id2label
    return id2label[predicted_class_id], confidence

# Beispiel
text = "Die Menschen in meinem Wahlkreis sind besorgt Ã¼ber die wirtschaftlichen Auswirkungen..."
frame, confidence = classify_frame(text)
print(f"Frame: {frame} (Confidence: {confidence:.3f})")
```

## Dependencies

```bash
# Basis-Dependencies
pip install duckdb transformers torch

# FÃ¼r OpenAI Fine-Tuning
pip install openai

# FÃ¼r Hugging Face Fine-Tuning
pip install datasets scikit-learn

# FÃ¼r Evaluation
pip install numpy pandas
```

## Tipps fÃ¼r bessere Ergebnisse

### Annotation
- **Konsistenz**: Verwende die gleichen Kriterien fÃ¼r Ã¤hnliche Texte
- **Confidence**: Gib ehrliche Confidence-Werte (1-5)
- **Notizen**: Dokumentiere schwierige FÃ¤lle
- **QualitÃ¤t**: Mindestens 50-100 Annotationen pro Kategorie empfohlen

### Fine-Tuning
- **DatenqualitÃ¤t**: Saubere, konsistente Annotationen sind wichtiger als Menge
- **Balance**: Versuche ausgewogene Verteilung der Kategorien
- **Validation**: Verwende Test-Set fÃ¼r unabhÃ¤ngige Evaluation
- **Iteration**: Verbessere Annotationen basierend auf Modell-Fehlern

### Modell-Auswahl
- **DistilBERT**: Schnell, gut fÃ¼r kleinere Datasets
- **BERT-base**: Ausgewogen zwischen Performance und Geschwindigkeit
- **RoBERTa**: Besser fÃ¼r komplexere Patterns
- **DeBERTa**: State-of-the-art fÃ¼r Text-Klassifikation

## Troubleshooting

### HÃ¤ufige Probleme

1. **Zu wenige Annotationen**
   - Mindestens 10-20 pro Kategorie
   - Verwende Few-Shot Learning fÃ¼r kleine Datasets

2. **Unausgewogene Daten**
   - Verwende stratifizierte Stichprobe
   - Oversampling fÃ¼r seltene Kategorien

3. **Schlechte Performance**
   - ÃœberprÃ¼fe Annotation-QualitÃ¤t
   - Experimentiere mit verschiedenen Modellen
   - ErhÃ¶he Training-Epochs

4. **Memory-Probleme**
   - Reduziere Batch-Size
   - Verwende Gradient-Checkpointing
   - Verwende kleinere Modelle

## Erweiterte Features

### Batch-Processing
```bash
# Verarbeite mehrere Chunks gleichzeitig
python scripts/load_speeches.py --max-speeches 100
```

### Custom Frames
Bearbeite `FRAME_CATEGORIES` in den Scripts fÃ¼r eigene Kategorien.

### Multi-Label Classification
Erweitere die Scripts fÃ¼r mehrere Labels pro Text.

### Active Learning
Implementiere Active Learning fÃ¼r effizientere Annotation.

## Lizenz

Dieses Projekt ist Teil des Brexit-Debatten-Analyse-Projekts.
